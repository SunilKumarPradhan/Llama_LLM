{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Chat with your PDF files using LlamaIndex, Astra DB (Apache Cassandra), and Gradient's open-source models, including LLama2 and Streamlit, all designed for seamless interaction with PDF files.\n",
        "\n",
        "[**Link to my YouTube Channel**](https://www.youtube.com/BhaveshBhatt8791?sub_confirmation=1)\n",
        "\n",
        "Click on the link below to open a Colab version of the notebook. You will be able to create your own version."
      ],
      "metadata": {
        "id": "sbzI6KKiDv78"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhattbhavesh91//pdf-q-a-llamaindex-llama2/blob/main/pdf-q-a-notebook.ipynb\" target=\"_blank\"><img height=\"40\" alt=\"Run your own notebook in Colab\" src = \"https://colab.research.google.com/assets/colab-badge.svg\"></a>"
      ],
      "metadata": {
        "id": "iW1MO9bXD964"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "XaiKzsSw4FBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q cassandra-driver\n",
        "!pip install -q cassio>=0.1.1\n",
        "!pip install -q gradientai --upgrade\n",
        "!pip install -q llama-index\n",
        "!pip install -q pypdf\n",
        "!pip install -q tiktoken==0.4.0"
      ],
      "metadata": {
        "id": "wak76xYYUdXE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebc04588-c684-4294-ae98-541e54b7e8ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.3/166.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.3/801.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.4/277.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import OS & JSON Modules"
      ],
      "metadata": {
        "id": "2iILdU7U4Hya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['GRADIENT_ACCESS_TOKEN'] = 'Enter your GRADIENT ACCESS TOKEN'\n",
        "os.environ['GRADIENT_WORKSPACE_ID'] =  'Enter your GRADIENT WORKSPACE ID'"
      ],
      "metadata": {
        "id": "U02ytLrPA2rG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Cassandra & llama Index"
      ],
      "metadata": {
        "id": "u3IS4xyg4N63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from cassandra.auth import PlainTextAuthProvider\n",
        "from cassandra.cluster import Cluster\n",
        "from llama_index import ServiceContext\n",
        "from llama_index import set_global_service_context\n",
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader, StorageContext\n",
        "from llama_index.embeddings import GradientEmbedding\n",
        "from llama_index.llms import GradientBaseModelLLM\n",
        "from llama_index.vector_stores import CassandraVectorStore"
      ],
      "metadata": {
        "id": "GhZ1NMr2z3vF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cassandra\n",
        "print (cassandra.__version__)"
      ],
      "metadata": {
        "id": "Q28fIUq91zED",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd2a8c3f-64bf-4271-b5dc-8aed147b2f7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connect to the VectorDB"
      ],
      "metadata": {
        "id": "cuio1UWlEMkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This secure connect bundle is autogenerated when you donwload your SCB,\n",
        "# if yours is different update the file name below\n",
        "cloud_config= {\n",
        "  'secure_connect_bundle': 'secure-connect-bhavesh-astra-test.zip'\n",
        "}\n",
        "\n",
        "# This token json file is autogenerated when you donwload your token,\n",
        "# if yours is different update the file name below\n",
        "with open(\"bhavesh_astra_test-token.json\") as f:\n",
        "    secrets = json.load(f)\n",
        "\n",
        "CLIENT_ID = secrets[\"clientId\"]\n",
        "CLIENT_SECRET = secrets[\"secret\"]\n",
        "\n",
        "auth_provider = PlainTextAuthProvider(CLIENT_ID, CLIENT_SECRET)\n",
        "cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)\n",
        "session = cluster.connect()\n",
        "\n",
        "row = session.execute(\"select release_version from system.local\").one()\n",
        "if row:\n",
        "  print(row[0])\n",
        "else:\n",
        "  print(\"An error occurred.\")"
      ],
      "metadata": {
        "id": "jYn5am9c1zGS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84ceb91e-ef43-46c7-b029-8ed6ebd3e75f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for ddcc6460-d494-4a85-9fca-fa397e733c54-us-east1.db.astra.datastax.com:29042:ef99c850-56be-415d-b21b-a5b234af6db9. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for ddcc6460-d494-4a85-9fca-fa397e733c54-us-east1.db.astra.datastax.com:29042:ef99c850-56be-415d-b21b-a5b234af6db9. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "ERROR:cassandra.connection:Closing connection <AsyncoreConnection(132061701584064) ddcc6460-d494-4a85-9fca-fa397e733c54-us-east1.db.astra.datastax.com:29042:ef99c850-56be-415d-b21b-a5b234af6db9> due to protocol error: Error from server: code=000a [Protocol error] message=\"Beta version of the protocol used (5/v5-beta), but USE_BETA flag is unset\"\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 5 to 4 for ddcc6460-d494-4a85-9fca-fa397e733c54-us-east1.db.astra.datastax.com:29042:ef99c850-56be-415d-b21b-a5b234af6db9. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.0.11-b86be92b8b5f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the Gradient's Model Adapter for LLAMA-2"
      ],
      "metadata": {
        "id": "1I8T-kjh4VoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = GradientBaseModelLLM(\n",
        "    base_model_slug=\"llama2-7b-chat\",\n",
        "    max_tokens=400,\n",
        ")"
      ],
      "metadata": {
        "id": "EksBmgQt36_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configure Gradient embeddings"
      ],
      "metadata": {
        "id": "CPTqOwXZ4aLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_model = GradientEmbedding(\n",
        "    gradient_access_token = os.environ[\"GRADIENT_ACCESS_TOKEN\"],\n",
        "    gradient_workspace_id = os.environ[\"GRADIENT_WORKSPACE_ID\"],\n",
        "    gradient_model_slug=\"bge-large\",\n",
        ")"
      ],
      "metadata": {
        "id": "38_uBhIy2XtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "service_context = ServiceContext.from_defaults(\n",
        "    llm = llm,\n",
        "    embed_model = embed_model,\n",
        "    chunk_size=256,\n",
        ")\n",
        "\n",
        "set_global_service_context(service_context)"
      ],
      "metadata": {
        "id": "mf7hu0cE2VKy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05efb65d-11e7-4464-e4c1-166a4927ebd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /tmp/llama_index...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the PDFs"
      ],
      "metadata": {
        "id": "T3AEoS-t4t6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader(\"/content/Documents\").load_data()\n",
        "print(f\"Loaded {len(documents)} document(s).\")"
      ],
      "metadata": {
        "id": "KFXZvFaQ25kE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38b0e43d-95f1-4c55-be13-62bfdc46fd2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 17 document(s).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and Query Index"
      ],
      "metadata": {
        "id": "Ns8r13uw4vko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(documents,\n",
        "                                        service_context=service_context)\n",
        "query_engine = index.as_query_engine()"
      ],
      "metadata": {
        "id": "YT9wD9sQ25nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"What is Positional Encoding?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "ZVYy24TR2VSM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04130f77-58f4-489c-ae7a-a0862ed22758"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.5 Positional Encoding is a technique used in the Transformer architecture to provide the model with information about the relative or absolute position of the tokens in the sequence. It is done by adding sinusoidal functions of different frequencies to the input embeddings at each position. The purpose of this technique is to allow the model to easily learn to attend by relative positions, and to extrapolate to sequence lengths longer than the ones encountered during training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nRyW0E5N4DiX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}